{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from math import log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('./project2_demo.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(line):\n",
    "    terms = line.split(' ')[1:]\n",
    "    return terms\n",
    "\n",
    "def filter_empty_strings(term):\n",
    "    return term != ''\n",
    "\n",
    "def extract_terms_with_doc_id(line):\n",
    "    data = line.split(' ')\n",
    "    doc_id, terms = data[0], data[1:]\n",
    "    return [term + \",,\" + doc_id for term in terms if term != '']\n",
    "\n",
    "def split_term_doc_id_pair(pair):\n",
    "    term, doc_id = pair.split(\",,\")\n",
    "    return (term, [doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count by Document based on doc id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_by_document = rdd.flatMap(lambda line: [[word + \"_\" + line.split(' ')[0], 1] for word in line.split(' ')[1:] if word != ''])\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda x: [x[0].split(\"_\")[1], [x[0].split(\"_\")[0],  x[1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency (First Hundred Entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_frequency = rdd.map(lambda line: [line.split(' ')[0], len(line.split(' ')[1:])])\\\n",
    "                    .join(frequency_by_document)\\\n",
    "                    .map(lambda pair: [pair[1][1][0], [pair[0], pair[1][1][1], pair[1][0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interdocument Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = rdd \\\n",
    "    .flatMap(lambda line: [[term, 1] for term in line.split(' ')[1:] if term != \"\"]) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(lambda x: [x[0], log(total_documents / x[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined TF IDF (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tf_idf(joined_pair):\n",
    "    term = joined_pair[0]\n",
    "    idf = joined_pair[1][1]\n",
    "    tf_doc = joined_pair[1][0][1]\n",
    "    doc_total_terms = joined_pair[1][0][2]\n",
    "    document_id = joined_pair[1][0][0]\n",
    "    return [term, document_id , idf * (tf_doc / doc_total_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sparse_matrix = term_frequency.join(word_counts)\\\n",
    "    .map(transform_tf_idf)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presennt mxn matrix by using dictionary with indices for document id and term index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can just do indexing into a matrix with assignments based on the term and document id in the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms  = rdd.flatMap(extract_terms).filter(filter_empty_strings).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_ids = rdd.map(lambda line: line.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_map = all_terms.zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = all_doc_ids.zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(data, term_map, document_map):\n",
    "    matrix = np.zeros(shape=(len(document_map), len(term_map)))\n",
    "    for (term, doc_id, value) in data:\n",
    "            doc_index = document_map[doc_id]\n",
    "            term_index = term_map[term]\n",
    "            matrix[doc_index][term_index] = value\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = make_matrix(tf_idf_sparse_matrix, terms_map, doc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = mat.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'growth'\n",
    "term_index = terms_map[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [np.sqrt(np.sum(np.square(mat[i]))) for i in range(len(terms_map))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [np.multiply(mat[term_index], mat[i]) for i in range(len(n)) if i != term_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [/ (n[term_index] * n[other_term_index]) for other_term_index in range(len(n)) if n != term_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
